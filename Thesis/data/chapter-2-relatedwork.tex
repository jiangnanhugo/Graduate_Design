\chapter{相关技术介绍}
在自然语言处理研究领域中，基于神经网络的分布式表示（Distributional Representation）通常被称作为词向量（Word Vector）、词嵌入（Word Embedding）或者分布式表示（Distributed Representation）~\upcite{DBLP:conf/nips/MikolovSCCD13}。基于神经网络的词向量表示技术是通过对语言模型的两个组成部分进行建模，即：1）单词的语境，也称为上下文（Context）；2）以及上下文与下一步的目标单词之间的关系（Next-Utterance Prediction）。
神经网络结构和组合方式相对灵活，可以表示更加复杂的上下文语义结构（Complex Context）。
历史上的词向量表示方法主要是基于单词共现矩阵（Co-occurrence Matrix），仅仅是利用了单词的顺序信息。
单词的语义向量是通过将单词共现矩阵进行矩阵分解（Matrix Decomposition），例如奇异值分解算法（Singular Value Decomposition，SVD）、潜在语义分析模型（Latent Semantic Analysis，LSA）或者潜在语义索引（Latent Semantic Indexing，LSI)等分解算法。其中对于SVD分解算法，第一个分解获得的矩阵就是我们需要的单词的词向量矩阵。
N-gram~算法也是使用单词的顺序信息作为上下文信息建模策略，当~$N$~线性增加时，N-gram~模型所需要存储的单词序列组合总数会呈指数爆炸增加，即维数灾难问题（Curse of Dimensionality）\footnote{https://en.wikipedia.org/wiki/Curse\_of\_dimensionality}，它深深限制了~N-gram~模型的实际应用。
假若使用神经网络替代~N-gram~模型来表示单词概率分布，当~$N$~线性增加时神经网络语言模型所需要的参数数量仅以线性速度增长。
这样我们就可以利用神经网络模型可以对更复杂的文本结构进行建模，在从训练后的模型获得的词向量中，将包含更多的语义信息、结构知识。

到目前为止，在语言模型的领域中，研究者提出的神经网络模型主要包括： 传统前向传递神经网络（Feed Forward Neural Network, FFNN）、循环神经网络（Recurrent Neural Network, RNN）三种基本的建模方案。当然基于这些基本组件，我们还可以构造更复杂的模型结构，但是无法保证复杂模型一定在效率和精确度上优于简洁的模型结构。复杂的模型可以表征更复杂的本文语义信息，例如句子的依存关系（Dependency）结构或者句子的句法分析树（Syntactic Structure）结构，但是这样的效果不一定能保证模型的泛化性能（Generalization）会很好，因为日常文本的语序是存在较多混乱的数据，很难找到良好定义的句子关系，所以也无法让模型从混乱的文本分布中学习到有效的句子结构，以提升模型的精度。显然是假设越弱模型越好，因为针对复杂的网络文本数据，我们模型训练所需要的数据质量相对较低，模型越容易收敛并获得有效的结果。模型假设越强，模型所需要的数据质量越高，实际稳定性就会降低，产生得不偿失的后果。


除此以外，针对在本论文所讨论的语言模型的大规模应用过程中所遇到的大词表问题，历史文献中提出的解决方法主要可以划分为：单词拆分算法（Vocabulary Truncation）、采样估计模型（Sampling-based Approximation）和词表层次分解（Vocabulary Factorisation），我们会在接下来的章节陆续介绍。其中本论文重点是研究基于类别的多元分类模型（class-based hierarchical softmax， cHSM）和基于二叉树的二元分类模型（class-based hierarchical softmax，tHSM），这两种算法将在下一章更详细讨论和介绍。

\section{语言建模}
在这一节中，我们将首先从语言模型提出的实际背景的形式化定义开始，以阐述语言模型的训练目标和实际应用场景~\upcite{DBLP:journals/tnn/ChienK16}。一个好的语言模型应当考虑对文本的两种特征进行建模：语法特征（Syntactics）和语义特征（Semantics）。为了保证句子的语法正确，我们需要考虑的是所生成的单词的前面的上下文（Previous Context），这也就意味着语法特性往往只对局部特征（Local Representation）进行建模。而语义特征的一致性则复杂了许多，它意味着我们需要考虑更大、更完善的上下文信息乃至整个文档语料，来获取全局语义（Global Context）而不仅仅是局部语义信息（Local Context）。


具体地来说，给定一个包含$T$个单词的序列：$w_1,w_2,\cdots,w_T$，这个序列的概率或者对数概率（Log-probability），即描述该序列作为一个合理且合法的句子的流畅性（Fluency）和出现该单词组合的句子的可能性（ Likelihood）。由于直接求解整个序列的概率比较复杂，所以我们将采用马尔可夫链规则（Markov Chain）~\footnote{https://en.wikipedia.org/wiki/Markov\_chain}，分解成一系列的条件概率（Conditional Probability）的联合分布（Joint Distribution）：
\begin{equation}\label{laguage_model}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
 \log p(w_1,\cdots, w_T ) = \sum_{t=1}^T \log p(w_t | w_{1:t-1}),
\end{equation}
其中前 $t-1$个单词记作$w_ {1:t-1}$。进而，条件概率~$p(w_t | w_ {1:t-1})$表示给定其前面上下文$w_ {1:t-1} $作为预测的下一个单词的条件概率分布（Conditional Probability）。最早的传统方法可以由前馈网络来建模这个上下文信息， 同时用 softmax 函数来表示下一个单词的概率分布计算公式~\upcite{DBLP:journals/jmlr/BengioDVJ03}。 在训练的过程中，整个模型以最小化每个单词预测的交叉熵损失（Cross Entropy Loss）为目标：
\begin{equation}\label{equ:losses2}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
  \ell=\sum_{t=1}^{T}\ell_t=\sum_{t=1}^{T}\log p(w_t | w_{1:t-1})
\end{equation}
其中文本或者句子的交叉熵代价函数$\ell$ 的实际意义为：是当编码方案不一定完美时，平均编码长度是多少（最优的编码长度就是1）。这里的编码长度的概念可以理解为：如何用01字节对所有的单词进行最短的编码，保证单词之间两两不互相冲突。交叉熵代价函数是由$\texttt{KL}$散度（Kullback–Leibler Divergence），也叫做相对熵（Relative Entropy），推导而获得的。这里我们需要注意的是，$\mathtt{KL}$散度的计算公式是：
\begin{equation}\label{equ:losses}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
  \mathtt{KL}(p||q)=-\sum_{t=1}^{T}p(x)\log q(x) - (\sum_{t=1}^{T}p(x)\log p(x))
\end{equation}
其中$p(x)$ 指的是文本数据的真实概率分布，$q(x)$ 是由模型从数据中计算得到的预测概率分布。由于$p(x)$ 和$q(x)$ 在公式中的地位不是相等的，所以$\texttt{KL} (p\parallel q)\not\equiv \texttt{KL}(q\parallel p)$。对于给定的文本语料训练数据集，第二项的计算值是常数，由此可知其导数是0，它不会对模型中的参数更新产生任何影响。所以在通常情况下，我们采用交叉熵函数$\ell$作为模型的代价函数而不是相对熵代价函数$\mathtt{KL}$，但其实我们希望模型优化的目标是$\mathtt{KL}$使得模型的分布拟合实际的分布。
\subsection{语言模型的应用}
语言模型的实际用途包括分词（Word Segmentation或Word Breaker，WB）, 信息抽取（Information Extraction，IE）, 关系抽取（Relation Extraction，RE）, 命名实体识别（Named Entity Recognition，NER）, 词性标注（Part Of Speech Tagging，POS）, 指代消解（Coreference Resolution）, 句法分析（Parsing）, 词义消歧（Word Sense Disambiguation，WSD）, 语音识别（Speech Recognition）, 语音合成（Text To Speech，TTS）, 机器翻译（Machine Translation，MT）, 自动文摘（Automatic Summarization）, 问答系统（Question Answering）, 自然语言理解（Natural Language Understanding）, 光学字符识别（Optical Character Recognition，OCR）, 信息检索（Information Retrieval，IR）\upcite{DBLP:journals/jair/GattK18}。

当我们对这些纷繁复杂的应用作一定抽象之后，可以将语言模型的主要作用分为以下三种情况：1） 为句子打分（Sentence Scoring）。给定一个未知的单词序列，我们需要计算其作为一个句子的流畅性和出现的可能性；2） 挑选最佳的候选词（Next-utterance Prediction）。给定上下文，选择概率最高的单词。这里所指的上下文的形式是广义的，它可能是：前面的句子给定，选择下一个最佳单词；前面和后面的句子给出，选择中间最适配的单词；后面的句子给定，选择上一个最佳单词（逆序文本建模也是有其独特应用地位的）；3） 训练词向量（Pretrained Word Embedding）。语言模型的输入，在经过大规模数据集训练之后，可以作为词向量输出，从而为其他文本任务提供有效的单词预训练特征。

\subsection{长距离依赖}
不同于前馈神经网络语言模型，循环神经网络语言模型使用隐藏层状态（Hidden State）来记录单词的顺序性信息（Sequence Order），它能够捕捉句子中存在的长距离依赖关系（Long-term Dependency）。这里的长距离依赖主要指语义对应关系（Semantic Correspondence）和选择限制（Selectional Preferences）\upcite{贾玉祥2014汉语语义选择限制知识的自动获取研究}。

一方面，两个单词距离较远但是 在文本中会存在很强的相关性关系。这里的所指的相关性，包括两个单词具有语义对应关系，或者他们需要满足句子的语法结构（Grammatical Structure）。如果采用传统的~N-gram~建模策略，我们必须要建模N-gram（$N>3$）单词概率分布，才能保证模型能学习到这种对应关系。

\begin{figure}[!b]
  \centering
  \includegraphics[width=.99\columnwidth]{./figures/lm.pdf}
  \caption{循环神经网络语言模型图例}
  \label{fig:lm}
\end{figure}

另一方面，文本中存在的依赖关系将会限制我们选择接下来的单词候选集，称其为选择限制关系。
如图~\ref{fig:lm}~所示，模型在预测过程中，在每一步都需要对词表中所有单词做预测，我们需要将每一步都做选择限制，减少模型的预测分支，这样才能保证模型生成的结果尽可能的语义一致且连贯。每次开放式的对所有单词做预测所带来的后果可能是，那些高频单词会不断出现在预测序列里面，因为训练语聊里面这些单词出现频率就非常高，所以模型将这些高频词预测概率大也是合乎情理的。但问题出现在，一旦我们模型不断预测高频词，我们生成的句子很难传达有效的语义信息。

\subsection{循环神经网络}
考虑到上面描述长距离依赖问题，在本实验中，我们将采用的基准模型是循环神经网络语言模型。近年来，基于时间序列建模的循环神经网络已经非常流行。如图~\ref{fig:lm}~所示，由于在当前时间步 $t$的输出$w_{t+1}$正好是下一个时间步的输入$w_{t+1}$，所以将上一步的输出接到下一步的输入。当我们需要做文本生成时，才需要利用这样的自循环链接（Auto-associative Connection）。每个句子都需要用开始词（即，$\langle s\rangle$）和结束词（即$\langle / s\rangle$）标记进行封装。 在预测下一个单词 $\hat w_{t+1}$ 之前，需要用到前一时刻的隐藏状态$h_{t-1}$和当前读入的单词~$w_t$~来计算网络单元的输入$h_t$。
\begin{equation}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
  \hat w_{t+1}=\arg\max_{w\in\mathcal{V}} \mathtt{softmax}(Vh_t+d)
\end{equation}
从形式上讲，循环神经网络是一个参数化的非线性函数$\phi(w_t,h_{t-1})$，循环地将一系列向量映射到一系列隐藏状态。 将$\phi(w_t,h_{t-1})$函数应用于任何这样的序列，在每个时间步骤$t$产生隐藏状态$h_t$，如下所示：
\begin{equation}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
  h_t \leftarrow  \phi(W\theta^w_t + U h_{t-1} +c),
\end{equation}
其中$ W,U,V $是模型参数，向量~$\theta^w_t$~对应于源词~$w_t$~的词向量。参数~$U$~是随时间共享的，即：当我们需要计算 $h_t$，我们需要用到上一步的隐藏层的输出$h_{t-1}$。

自Elman提出基本结构~\upcite{DBLP:journals/cogsci/Elman90}~以来，许多扩展模型能学习长距离依赖关系，解决梯度消失（Gradient Vanishing）和梯度爆炸（Gradient Exploding）等问题， 如长短期记忆网络（LSTM）\upcite{7508408}、门限记忆单元（GRU）\upcite{DBLP:conf/nips/ChungKDGCB15}和近似递归神经网络（Quasi-RNNs）\upcite{DBLP:journals/corr/BradburyMXS16}。

\begin{figure}[!t]
  \centering
  \includegraphics[width=.8\linewidth]{./figures/lstm.pdf}
  \caption{LSTM 模型示意图}\label{fig:lstm}
\end{figure}

长短记忆网络（LSTM）是根据RNN 所改进的最著名的模型之一，LSTM 模型的计算公式定义如下\upcite{DBLP:journals/neco/HochreiterS97}： 1）输入门$i_t$ 控制当前输入 $x_t$ 和前一步输出 $h_{t−1}$ 进入新的 Cell 的信息量；2） 遗忘门$f_t$ 决定是否清除或者保持单一部分的状态；3） 变换输出和前一状态到最新状态~$g_t$；4） 输出门$o_t$计算 Cell 的输出 ；5） cell 状态更新$s_t$。其计算步骤是，计算下一个时间戳的状态使用经过门处理的前一状态和输入；6） 最终 LSTM 的输出$h_t$ 使用一个对当前状态进行 $\tanh$ 变换。其计算公式如下：
\begin{equation}\label{equ:lstm}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{split}
   i_t&=\sigma(W^i x_t+U^i h_{t-1}+b^i) \\
   f_t&=\sigma(W^f x_t+U^f h_{t-1}+b^f) \\
   g_t&=\phi(W^g x_t+U^g h_{t-1}+b^g) \\
   o_t&=\sigma(W^o x_t+U^o h^{t-1}+b^o) \\
   s_t&=g_t\odot i_t+s_{t-1}\odot f_t,\quad h_t=s_t\odot \phi(o_t)
\end{split}
\end{equation}
其中$\odot$ 代表对应元素相乘（Element-wise Matrix Multiplication）， 函数 $\phi(x), \sigma(x)$ 的定义：
\begin{equation}\label{equ:tanh}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
  \phi(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}},\sigma(x)=\frac{1}{1+e^{-x}}
\end{equation}
在具体算法实现过程中，LSTM模型由于参数之间相似性和独立性，我们可以并行计算实现该模型。具体来说，模型的输入门、遗忘门、输出门和状态更新门所涉及的矩阵乘法操作我们可以同时计算，从而提高LSTM模型的计算效率\upcite{陈凯2016深度学习模型的高效训练算法研究}。因为LSTM的计算模型更容易并行，所以他与GRU的计算时间相差无几，尽管LSTM的模型需要的参数量是GRU模型的1.5倍。
\begin{equation}\label{equ:weight}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
[i^t,f^t,g^t,o^t ]=[\sigma, \sigma,\phi,\sigma]\times W\times[x_t,h_{t-1}]^\top
\end{equation}
其中 $W$ 指的是8个小矩阵互相层叠起来，即 $[[W^i,W^f,W^g,W^o],[U^i,U^f,U^g,U^o]]$。


\begin{figure}[!t]
  \centering
  \includegraphics[width=0.6\linewidth]{./figures/gru.pdf}
  \caption{GRU模型示意图}\label{fig:gru}
\end{figure}

接下来我们介绍基于LSTM简化的模型，门限记忆单元（GRU）把 LSTM模型中的遗忘门$f_t$和输入门~$o_t$~用更新门$z_t$ 替代。同时，它把~Cell~的状态值和隐状态~$h_t$~进行了合并。 图~\ref{fig:gru}~是GRU更新~$h_t$~的过程\upcite{DBLP:journals/corr/Pezeshki15}, 主要包括：更新门~$z_t$、 重置门 $r_t$ 和节点内部更新值$\tilde h_t$和隐藏层输出值~$h_t$， 这里与~LSTM~的区别是GRU中没有输出门，其计算公式如下：
\begin{equation}\label{equ:gru}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{split}
   z_t &= \sigma ( W^z x_t+ U^z h_{t-1}) \\
   r_t &= \sigma(W^r x_t  + U^r h_{t-1}  )\\
   \tilde h_t  &= \tanh (W^h x_t  + U^h(h_{t-1} \odot r_t) ) \\
   h_t &= (1-z_t)\odot \tilde h_t  + z_t \odot h_{t-1}
\end{split}
\end{equation}
如果重置门~$r_t=0$，那么计算~$h_t$~时将会丢弃之前的所有信息，即：$\tilde h_t=\tanh W^h x_t$； 若~$z_t=1$，模型直接把前一刻隐藏层输出~$h_{t-1}$~复制到当前时刻~$h_t=h_{t-1}$；如果~$r_t=1,z_t=0$，则~GRU~模型变成了一个传统的~RNN~模型，它只能处理短距离依赖问题。


\section{大词表问题}
softmax~函数通常用于多标签分类（Multi-class Classification）问题中的概率归一化（Probability Normalization）。考虑到~softmax~函数中包含除法操作，容易导致模型求解导数出现~NaN（Not-a-Number）现象，通常直接计算log-softmax函数。然而当分类的目标变多时，即对于语言模型的大词表问题中，softmax~函数会占用绝大部分计算时间，需要我们对这个计算瓶颈做更详细分析。这里我们首先给出~softmax，log-softmax~函数和相应的梯度（Gradient）的定义：
\begin{equation}\label{eq:softmax}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{split}
p(w|h)=&\frac{\exp(h^\top v_{w})}{\sum_{w_j\in \mathcal{V}}{\exp(h^\top v_{w_j} )}} \\
\frac{\partial p(w_i|h)}{\partial v_{w_j}}=&p(w_j|h)(\delta_{ij}-p(w_i|h))h^\top\\
\end{split}
\end{equation}
\begin{equation}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\label{eq:log-softmax}
\begin{split}
\log p(w|h) &= \theta^w h-\log \sum_{u\in \mathcal{V}}{\exp(\theta^u h)}\\
\nabla_{\theta^u}{\log p(w|h)}&= (\delta_{uw}-p(w|h))h
\end{split}
\end{equation}
其中~$h$~指代的是隐藏层输出的向量（即上下文表示），$\theta^w$~表示单词~$w$~的目标单词向量~\upcite{duda2012pattern}。
公式中的克罗内克~$\delta$~函数~$\delta_{uv}$（Kronecker Function）定义为：如果~$u,v$~指的是相同的单词结果是~$1$，如果~$u,v$指的不是相同的单词则是~$0$。第一行计算公式指的是~softmax~函数，第二行计算公式指的是~softmax~函数的导数，第三行计算公式指代的是~log-softmax~函数，第四行计算公式指代的是~log-softmax~的导数。


如公式~(\ref{eq:softmax})~和~(\ref{eq:log-softmax})~所示，前向概率传播函数~$\log p(w|h)$（即第一行方程）和后向梯度优化函数${\partial p(w_i|h)}/{\partial v_{w_j}}$（即第二行的方程）都需要计算目标词汇表中的所有单词，因此在该部分上花费的时间会随着词表中包含的单词数量的增加而线性地增长~\upcite{DBLP:conf/acl/ChenGA16}。举例来说，假设词表中包含~$\mathcal{|V|}$~数量的单词来说，总的时间复杂度（Time Complexity）是~$\mathcal{O}(\mathcal{|H||V|})$，其中~$\mathcal{|H|}$~表示的是隐藏层输出的维度。代码实现过程中，softmax~函数被分解为：计算矩阵乘法（Multiply），计算求和函数（Summary），计算概率归一化的除法函数（Divide），第一步乘法操作占用了主要的计算时间。而且，目前的~GPGPU~设备的显存（Graphical Memory）是相当有限的，通常~Nvidia~公司提供的设备为~12GB~显存，目前最大的计算设备~Nvidia-K80~显存为24GB，然而这个设备造价相当昂贵\footnote{https://www.nvidia.com/zh-cn/data-center/tesla/}。因为我们不能任意增大显存大小，所以对词表大小有一个天然的上限。另一种缓解大词表存储问题的方法是进行多显卡训练，将大规模词表矩阵存储在单独的显存上。

我们需要区分~softmax~概率计算瓶颈是来自求和函数还是矩阵乘法函数。
softmax~算法的计算瓶颈不能归因于方程~\ref{eq:softmax}~中的循环函数~$\sum_ {u \in \mathcal {V}}$，尽管它随着词汇量的增长而线性增长，但是矩阵和张量的乘法计算的更消耗时间。所以相比于循环函数而言，主要的计算时间用于矩阵乘法计算。所以只有那些避免大矩阵乘法运算，即避免计算其他冗余字的概率才能达到最大的速度收益，而那些仍然涉及全局概率归一化的方法，并不能真正改善~softmax~占用的计算时间，那些优化求和函数的计算效率所获得的实际收益是微乎其微。


因此，在保证模型计算准确度不下降的情况下，我们期望能缓解概率归一化函数的计算瓶颈进而提高模型的训练速度。 目前能有效缓解大词表问题的算法主要分为以下三类： 单词拆分算法（Vocabulary Truncation）、采样估计模型（Sampling-based Approximation）和词表层次分解算法（Vocabulary Factorization）。下面将对三种不同思路的算法进行详述。



\subsection{单词拆分算法}
针对大词表问题，最直接、最简单的策略就是我们放弃使用大词表，转而保留较小的高频词来保证训练和测试的时候模型的内存（Memory Footprint）占用小和计算效率高。当我们只保留高频词，就会剩下很多其他不在词表中的单词被映射成~$\langle$unk$\rangle$。这相当于直接丢弃部分输入信息，可能使模型的训练变差。那么针对剩余的过多的词表外的单词（Out-Of-Vocabulary，OOV）， Schwenk~等人提出可以使用传统的~N-gram~语言模型来估算其可能的概率分布~\upcite{DBLP:journals/csl/Schwenk07}。
这样做一方面保证神经网络模型可以在有限时间内训练完，同时保证模型的最后的测试结果不会很差。

然而我们需要注意到，当我们的词表继续减少并且我们选取的训练样本包含的不同的单词数量不断增加的时候，我们会发现训练样本中存在过多的~$\langle$unk~$\rangle$~字符，这直接导致转换后的句子变成了无法阅读者理解的句子，导致输入模型的信息噪声非常大，使得神经网络的模型训练非常困难，进而导致模型效果变得非常地差。所以这种方案只是一定程度上缓解了大词表问题，但是不失为一种简单且有效的尝试方案。

除了上述直接的建模方案外，目前采用的方案是将单词按照字符级别来划分。英文单词是由多个字符（Character）组成的，可以将一个单词按照字符统计规律划分成任意多个子词（Sub-word）。例如，Tucker~等人提出利用二元对编码（Byte Pair Encoding，BPE）将一个单词划分成前缀（Prefix）和后缀（Suffix）两部分~\upcite{DBLP:conf/icassp/Tucker0P94, DBLP:conf/acl/SennrichHB16a,Gage:1994:NAD:177910.177914}，如图~\ref{fig:subword} 所示。从该图中我们可以看出，单词被划分成前缀和后缀两部分，其中$+$代表是单词的前缀，同时~$\langle /w \rangle$~是单词的后缀。
由于划分规则是从训练数据中学习到的，我们可以指定需要缩减的词表大小，所以该算法的动态适应性很强。
最初~Kucker~等人提出该算法目的是文本信息压缩（Zip软件），后来~Sennrichr~等人将该算法应用在目前主流的机器翻译模型中，获得很好的精度和性能收益~\upcite{DBLP:journals/corr/JozefowiczVSSW16}。
后续的提出的算法中，直接将单词拆分成字符序列（英文字符、数字和标点符号总共包含94个符号），模型训练得到的将不再是词向量而是字符向量（Character Embedding）。在测试的时候，我们将通过组合子词向量或者字符向量来获得这个单词的实际词向量。尽管如此，我们仍然需要看到它这样将单词解构成前缀和后缀的操作依然带来了一定的损失，因为句子的长度加倍对循环神经网络学习长距离关系能力提出了更高的挑战~\upcite{DBLP:conf/aaai/KimJSR16}。

\subsection{采样估计模型}
第二种策略也是通过减少单词数量从而减少计算量。与上面直接将单词拆分成子词不同的是：这种算法是在训练的时候，通过采样算法选择少量单词，用来估计softmax~函数的可能的概率分布。因为实际只有一个单词是需要预测的，其他的单词都是可以认为是噪声单词。

目前流行的采样算法（Sampling Algorithm）主要是针对公式~(\ref{eq:softmax})~中对所有单词概率规约那一项进行概率估计，可以分类为：重要性采样（Importance Sampling）~\upcite{DBLP:journals/tnn/BengioS08}，噪声差分估计（Noise Contrastive Estimation，NCE）~\upcite{DBLP:conf/icml/MnihT12}和~Blackout~采样算法~\upcite{DBLP:journals/iclr/JiVSAD15}。第一种算法在~Bengio~等人的论文实验中被证明模型无法收敛~\upcite{DBLP:journals/tnn/BengioS08}，因此目前主流模型都使用后面的两种采样算法，所以我们仅对后面两种方法进行概述。

对于~NCE~噪声差分估计算法来说，模型的学习目标是区分将正确的单词~$w_0$~与随机生成的单词~$\{w_1\cdots w_k\}$。
其中~$w_0$～指代的是训练样本中真正的下一个单词，$\{w_1\cdots w_k\}$~是采用先验分布~$q(w)$~产生的随机噪声单词。实际实验中，我们常用的采样的概率分布模型~$q(w)$~是单词词频分布（Word Frequency Distribution）。正例归一化后的概率~$\tilde{p}(y=1|h)$~和所有负例联合概率~$\tilde{p}(y=0|h)$~的公式可以写成如下形式：
\begin{equation}\label{equ:nce}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{split}
  \tilde{p}(y=1|h)=&\frac{\exp( \theta^w_0 h)}{ \exp( \theta^w_0 h)+k *q(w_0)}\\
  \tilde{p}(y=0|h)=&\prod_{i=1}^{k}\frac{k *q(w_i)}{\exp( \theta^w_i h)+k *q(w_i)}\\
\end{split}
\end{equation}
噪声概率~$\tilde{p}(y=0|h)$~仅对~$k$~个噪声样本做加和运算而不是对整个词表进行运算的，因此该算法的计算复杂度就是~$\mathcal{O}(k+1)$，所以该算法与简单的~softmax~算法计算复杂度的比值是~$\mathcal{O}(\mathcal{|V|}/(k+1))$。

除了上面这种最经典的算法之外，后续又衍生出了其他的采样算法。
最近才提出的~Blackout~采样算法针对噪声概率归一化的时候~$\{w_1\cdots w_k\}$~与当前上下文的关系，对上述的~NCE~算法进行了进一步优化和修正~\upcite{DBLP:journals/iclr/JiVSAD15}，同时也对模型引入了更多的计算量，计算公式如下所示：
\begin{equation}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{split}
&\ell=-\log(p(w_0|h)) - \sum_{w_i \sim q(w)} \log(1 - p(w_i))\\
&p(w_i) = \frac{\exp(\theta^w_i h)}{\sum_{w_i \sim q(w)} \exp(\theta^w_i h)}
\end{split}
\end{equation}

总的来说，这些采样近似算法可以显著加快训练速度，但仍然需要利用单词分布$q(w)$~\upcite{DBLP:conf/naacl/ZophVMK16}来采样大量的噪音单词，在采样单词足够多的情况占用的时间仍然很可观，需要进一步的手段去优化。
另一方面，在推理测试时无法预知单词的概率分布更无法得知正确的单词是什么，只能调用~softmax~函数计算单词概率分布，意味着该算法无法应用在测试推理阶段~\upcite{DBLP:journals/jmlr/GutmannH12}。
同时，由于在做采样估计时引入了采样函数~$q(w)$，模型将会花费额外时间在采样计算上，因此我们需要寻找一个性能优异的离散采样函数（Discrete Sampling Function）或者是带权重的随机数生成器（Weight Random Number Generator）。


\subsection{词表层次分解}
介绍完单词拆分、采样估计算法后，第三种层次分解方法（Factorization）可以有效降低训练和推理过程中计算概率分布所占用的内存，因为它只计算局部概率（Local Probability）并且选择每一层的分数最高的候选路径而不是保存全局计算结果。
目前主要的分解策略可以分为：基于类别的多元分类模型（Class-based Hierarchical Softmax，cHSM）和基于二叉树的二元分类模型（Tree-based Hierarchical Softmax，tHSM）。接下来我们介绍这两种算法的计算过程。

首先我们考虑基于类别的分解策略。假设文本所对应的词表被划分成各个类簇，每个单词只属于一个类（Class或者Group）。在此假设之上，单词在在语料中的概率分布可以转化为：先计算类别的概率分布，然后乘以在所属类上该词的概率分布~\upcite{王龙2015基于循环神经网络的汉语语言模型并行优化算法}~，于是可将公式~(\ref{eq:softmax})~转化为如下形式：
\begin{equation}
\setlength{\abovedisplayskip}{10pt}
\setlength{\belowdisplayskip}{10pt}
\begin{split}
&p(w|h)=p^c(\mathcal{C}(w)|h)\cdot p^w(w|\mathcal{C}(w),h) ,\\
 & w\in \mathcal{C}(w),\mathcal{V}=\bigcup _{i = 1}^\mathcal{C}{c_i}, \quad  c_i \bigcap c_j=\phi, \text{若}\quad i\ne j, \\
\end{split}
\end{equation}
其中第一项概率~$p^c(\mathcal{C}(w)|h)$~表示每个类别的概率，第二项概率~$p^w(w|\mathcal{C}(w),h)$~表示在类别$\mathcal{C}(w)$中所有单词的局部概率。

该模型计算一个词的概率分布所需要的计算复杂度正比于: $\mathcal{O =|H||C|}$。 在该计算公式中，$\mathcal{C}$~表示的是该语料中所有词的分类数目，我们可以按照不同策略划分词表，最简单的一种策略就是根据语料中词的词频进行均匀划分。 当$C$ 取$1$ 或取词表大小~$|V|$~时，此结构等同于标准的~softmax~结构，此时词表还是原来的结构，并没有任何优化效果。因此，对于绝大多数情况，我们取$\mathcal{C} \ll \mathcal{V}$并且 $|\mathcal{C}|\ll 1$，这样的结构才能保证有效降低~softmax~的计算复杂度。图~\ref{fig:case_hsm}~展示了类别划分和分步概率计算的过程。如图图~\ref{fig:case_hsm}~所示，词表 [duck,cat,mop,broom,the,am] 被划分成三个类别：$c_1\to$[duck,cat]，$c_2\to$[mop,broom]并且$c_3\to$[the,am]。
\begin{figure}[!t]
  \centering
\includegraphics[width=.9\linewidth]{./figures/case_chsm.pdf}
\caption{cHSM算法可视化模型}\label{fig:case_hsm}
\end{figure}

接下来分析基于类别的分层模型计算速度提升效果。假设每个类包含~$\sqrt{\mathcal{|V|}}$ 单词，表示词汇被划分为相同大小的组，这样的话级联概率（Cascaded Probability）只涉及~$2\sqrt{\mathcal{|V|}}$~单词 softmax计算，所以最优时间复杂度可以减少到~$\mathcal{O}(\mathcal{|H|}\sqrt{\mathcal{|V|}})$~\upcite{DBLP:conf/icassp/Goodman01}。虽然在更常见的情况下，词表划分算法会产生不同大小的分组，这需要其他结构来支撑这种非均匀划分操作，尚未被完全探索和实验分析。此外，我们还可以通过调整不同的类数目和词表划分算法来调整模型的精度和效率。

另一方面，我们再介绍基于二叉树分解词表的建模策略。tHSM~方法将单步骤分类过程分解为多步骤二元分类步骤。正因为如此，词汇组织为二叉树，其中单词分布在二叉树的叶子节点上，二叉树的所有的中间节点的都是内部参数。在该二叉树是平衡树（Balanced Binary Tree）的结构情况下，每个词将会被赋予相等长度的0，1路径，最优时间复杂度可以减少到$\mathcal{O(|H|\log \mathcal{|V|})}$。图~\ref{fig:case_thsm} 展示了一颗平衡二叉树词表分解和访问路径的示例。关于如何构建这颗二叉树，最早的工作中单词在二叉树上的分布可以由WordNet与领域专家~\upcite{DBLP:conf/aistats/MorinB05}或由单词~Uni-gram~分布~\upcite{DBLP:conf/nips/MikolovSCCD13}推导的霍夫曼编码构建。然而，在大规模文本应用中，专家知识的构建是相当昂贵的。所以，一般不会采用这样的方案来构建单词的二叉树分布，霍夫曼编码方案只考虑单词词频信息，而单词的句法或语义信息尚未被利用和得到彻底的分析讨论。

\begin{figure}[!t]
  \centering
\includegraphics[width=.85\linewidth]{./figures/thsm-example.pdf}
\caption{tHSM算法可视化模型}\label{fig:case_thsm}
\end{figure}

\section{离散采样算法}
\begin{figure}[!b]
  \centering
\includegraphics[width=1\linewidth]{./figures/cdfreverse.pdf}
\caption{由离散概率分布构造累积概率分布}\label{fig:cdf_reverse}
\end{figure}

前文提到在使用基于采样估计模型时，不可避免地需要使用到离散采样函数，我们在这一个部分来讨论构建高效离散采样函数的策略。首先给出离散采样问题的定义：设一共有~$|\mathcal{V}|$~个单词，第~$i$个~单词~$w_i$~出现的概率是~$p(w_i)$， 如何高效地产生这样的随机变量序列~$w_1,w_2,\cdots$?该问题被称之为模拟离散取样（Simulated Discrete Sampling），其含义指的是根据有限类别的非均匀概率分布，来模拟有放回的采样，从而生成需要的采样序列。因为计算机只能快速产生伪随机数（Psudo Random Number Generator），即对均匀分布（Uniform Distribution）做采样，其他的复杂采样函数均建立在均匀分布的变换之上。所以该问题的关键就是如何将均匀分布变换到实际变量的分布，生成指定概率分布的随机单词序列。

如图~\ref{fig:cdf_reverse}~所示，首先将离散概率分布转换成累积分布函数表（Cumulative Distribution Function，CDF）。我们可以使用Matlab代码，很容易地完成这个计算过程。如图~\ref{fig:sample}~所示，所展示的代码使用了线性搜索算法(Linear Search)。该算法很简单，最容易实现。但是注意到该算法初始化的时间复杂度是~$\mathcal{O(\mathcal{V})}$~。计算每个样本时的时间复杂度也是~$\mathcal{O(V)}$。
\begin{figure}[!t]
  \centering
\includegraphics[width=1\linewidth]{./figures/cdf.pdf}
\caption{Matlab实现的离散采样函数}\label{fig:sample}
\end{figure}


考虑到该累积分布是从小到大顺序组织的，可以使用数据结构中的查找算法更快地找到结果。我们可以采用从左至右的方式线性搜索随机数所在的区间，其时间复杂度是$\mathcal{O}(\mathcal{V})$；也可以使用更高效的二叉搜索(Binary Search)快速计算随机数所在的区间，对应的时间复杂度是$\mathcal{O}(\log \mathcal{V})$。



在~$\mathcal{V}$~很小的情况下，使用线性查找算法或者二分搜寻算法也足够得快；当~$\mathcal{V}$足够的大，即所谓的大词表问题的时候，我们可以采用别名方法(Alias Method)\upcite{DBLP:reference/stat/LEcuyer11}、近似方法等~\upcite{DBLP:journals/cgf/ClineRW09}。本实验采用别名算法，该算法利用两次随机采样来生成采样序列，具有恒定的~$O(1)$采样时间复杂度。该算法的精髓在于构造别名表（Alias Table），需要~$O(\log |\mathcal{V}|)$~构造时间复杂度~\upcite{10.2307/2683739,DBLP:conf/emnlp/VaswaniZFC13}，具体构造方法可以参考\footnote{https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/}。

\begin{figure}[!t]
  \centering
\includegraphics[width=1\linewidth]{./figures/alias.pdf}
\caption{由离散概率分布构造别名表}\label{fig:alias}
\end{figure}


\section{本章小结}
本章对国内外学者在语言建模任务上的相关工作进行了概述。首先介绍了语言模型的建模策略和对应的实际应用场景。其次详细讨论了循环神经网络的语言模型的建模方式，并讨论了其结构的优缺点。同时为了解决使用循环网络所带来的梯度爆炸问题，我们进而引入了长距离短期神经网络和门限记忆节点两种基于门限机制的循环网络；另一方面，我们还考虑了语言模型大规模应用所带来的大词表问题，并作了详细分析原因。
同时，还讨论了历史上所提出的解决方案，主要涉及三种思路：缩减词表大小，用采样算法近似估计和模型层次分解，我们对各种模型的结构做了概述，并分析了各种算法的优劣。最后分析了实验中需要用到的高效采样函数的构造算法。
